{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of solar flares from sunspot data\n",
    "## Data \n",
    "The data-files are stored in the `flare.data1` and `flare.data2` files located in the directory `..\\data`.\n",
    "The data was pulled from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets.html), who in turn had it donated from Gary Bradshaw. \n",
    "\n",
    "## Attribute information\n",
    "Both datasets are supplied with ten input variables and three classes to be predicted. This problem can be thought of as a multivariate regression problem in that one tries to predict the number of occurences of multiple classes. The input-attributes are as follows: \n",
    "\n",
    "   1. Code for class (modified Zurich class)  (A,B,C,D,E,F,H)\n",
    "   2. Code for largest spot size              (X,R,S,A,H,K)\n",
    "   3. Code for spot distribution              (X,O,I,C)\n",
    "   4. Activity                                (1 = reduced, 2 = unchanged)\n",
    "   5. Evolution                               (1 = decay, 2 = no growth, 3 = growth)\n",
    "   6. Previous 24 hour flare activity code    (1 = nothing as big as an M1, 2 = one M1, 3 = more activity than one M1)\n",
    "   7. Historically-complex                    (1 = Yes, 2 = No)\n",
    "   8. Did region become historically complex  (1 = yes, 2 = no) on this pass across the sun's disk\n",
    "   9. Area(1 = small, 2 = large)\n",
    "  10. Area of the largest spot (1 = <=5, 2 = >5)\n",
    "  \n",
    "Initially the data is loaded up and preprocessed with the Pandas library. Since three of  the above attributes are categorical they need to be mapped to a numerical value. Pandas provides a method `get_dummies` that accomplishes this in a way that suits our purpose.\n",
    "\n",
    "## Skewedness\n",
    "One of the things that makes this dataset fundamentally interesting is that it's pretty _skewed_. From the repository we're informed that the datasets have class distributions as follows: \n",
    "\n",
    "**`flare.data1`** :   \n",
    "\n",
    "|Class    | 0   | 1   | 2   | 4   | Total |\n",
    "|:-------:|:---:|:---:|:---:|:---:|:-----:|\n",
    "|C flares |287  | 29  | 7   |  0  | 323   |\n",
    "|M flares | 291 | 24  | 6   | 2   | 323   |\n",
    "|X flares | 316 | 7   | 0   |  0  | 323   |\n",
    "\n",
    "**`flare.data2`** :\n",
    "\n",
    "|Class  |  0  |  1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | Total |\n",
    "|:-------:|:---:|:---:|:---:|:---:|:-----:|\n",
    "| C-class | 884 | 112 | 33 | 20 | 9 | 4 | 3 | 0 | 1 | 1066 |\n",
    "| M-class | 1030 | 29 | 3 | 2 | 1 | 0 | 1 | 0 | 0 | 1066 |\n",
    "| X-class | 1061 |  4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1066 |\n",
    "\n",
    "So much of our task will be to find reasonable and clever ways to treat this data. Either by way of sampling or transforming it in some manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation of results\n",
    "\n",
    "Before we jump into the nitty-gritty of the data it's important to have considered the way in which the data will be evaluated. \n",
    "\n",
    "### $R^2$\n",
    "The R-squared value of two datasets is a measure of their _correlation_. Useful since it allows a consideration of how closely one dataset follows another. In the function below this coefficient is computed by the python library `scipy`. For our use it will suffice to say that the $R^2$ value is bounded in the interval $[-1,1]$ where values closer to the end points of the interval signify higher degrees of correlation between the datasets.\n",
    "### P-values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(predicted_vals, y_matr):    \n",
    "    if isinstance(y_matr, pd.DataFrame):\n",
    "        y_matr = Y_test.as_matrix()\n",
    "\n",
    "    C_corr_p = pearsonr(predicted_vals[:,0], y_matr[:,0])\n",
    "    M_corr_p = pearsonr(predicted_vals[:,1], y_matr[:,1])\n",
    "    X_corr_p = pearsonr(predicted_vals[:,2], y_matr[:,2])\n",
    "\n",
    "    for flare_class, corr_p in zip([\"C\", \"M\", \"X\"], [C_corr_p, M_corr_p, X_corr_p]):\n",
    "        print(\"Flare class:  {} | Pearson R = {:.3f} | two - tailed p = {:.2e} \".format(\n",
    "            flare_class,\n",
    "            corr_p[0],\n",
    "            corr_p[1]))\n",
    "    return (C_corr_p, M_corr_p, X_corr_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>Evolution</th>\n",
       "      <th>Prev 24h act</th>\n",
       "      <th>Hist. complx.</th>\n",
       "      <th>Did reg. bec. complx.</th>\n",
       "      <th>Area</th>\n",
       "      <th>Area of lrgst. spt.</th>\n",
       "      <th>C-class - nxt 24h</th>\n",
       "      <th>M-Class - nxt 24h</th>\n",
       "      <th>X-class - nxt 24h</th>\n",
       "      <th>...</th>\n",
       "      <th>largest spot_A</th>\n",
       "      <th>largest spot_H</th>\n",
       "      <th>largest spot_K</th>\n",
       "      <th>largest spot_R</th>\n",
       "      <th>largest spot_S</th>\n",
       "      <th>largest spot_X</th>\n",
       "      <th>spot distr._C</th>\n",
       "      <th>spot distr._I</th>\n",
       "      <th>spot distr._O</th>\n",
       "      <th>spot distr._X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity  Evolution  Prev 24h act  Hist. complx.  Did reg. bec. complx.  \\\n",
       "0         1          3             1              1                      1   \n",
       "1         1          3             1              1                      2   \n",
       "2         1          3             1              1                      2   \n",
       "3         1          2             1              1                      1   \n",
       "4         1          1             1              1                      2   \n",
       "\n",
       "   Area  Area of lrgst. spt.  C-class - nxt 24h  M-Class - nxt 24h  \\\n",
       "0     1                    1                  0                  0   \n",
       "1     1                    1                  0                  0   \n",
       "2     1                    1                  0                  0   \n",
       "3     1                    1                  0                  0   \n",
       "4     1                    1                  0                  0   \n",
       "\n",
       "   X-class - nxt 24h      ...        largest spot_A  largest spot_H  \\\n",
       "0                  0      ...                     1               0   \n",
       "1                  0      ...                     0               0   \n",
       "2                  0      ...                     0               0   \n",
       "3                  0      ...                     0               0   \n",
       "4                  0      ...                     0               0   \n",
       "\n",
       "   largest spot_K  largest spot_R  largest spot_S  largest spot_X  \\\n",
       "0               0               0               0               0   \n",
       "1               0               1               0               0   \n",
       "2               0               0               1               0   \n",
       "3               0               1               0               0   \n",
       "4               0               0               1               0   \n",
       "\n",
       "   spot distr._C  spot distr._I  spot distr._O  spot distr._X  \n",
       "0              0              0              0              1  \n",
       "1              0              0              1              0  \n",
       "2              0              0              1              0  \n",
       "3              0              0              0              1  \n",
       "4              0              0              0              1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/flare.data2\", sep  = \" \", skiprows  = 1, header = None, \n",
    "                 dtype={\"0\": \"category\", \"1\": \"category\", \"2\": \"category\"})\n",
    "df.columns = [\"Class\", \"largest spot\", \"spot distr.\", \"Activity\", \"Evolution\", \"Prev 24h act\", \n",
    "             \"Hist. complx.\", \"Did reg. bec. complx.\", \"Area\", \"Area of lrgst. spt.\", \n",
    "             \"C-class - nxt 24h\",\n",
    "             \"M-Class - nxt 24h\",\n",
    "             \"X-class - nxt 24h\"]\n",
    "\n",
    "df_numerical = pd.get_dummies(df, columns = [\"Class\", \"largest spot\", \"spot distr.\"])\n",
    "df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1066"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial approach\n",
    "We'll start treating this dataset with naive approaches from machine learning and thought to the sampling of the data. Doing this we'll expect rather poor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df_numerical[:900]\n",
    "test = df_numerical[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train, Y_test= list(map(lambda x: x.filter(\n",
    "             [\"C-class - nxt 24h\",\n",
    "             \"M-Class - nxt 24h\",\n",
    "             \"X-class - nxt 24h\"],\n",
    "             axis = 1),\n",
    "            [train, test]))\n",
    "\n",
    "X_train, X_test = list(map(lambda x: x.drop(labels = [\"C-class - nxt 24h\",\n",
    "             \"M-Class - nxt 24h\",\n",
    "             \"X-class - nxt 24h\"],\n",
    "            axis = 1), [train, test]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees are a widely used algorithm both in regression and classification problems. Essentially they construct a hierarchy of if-else questions that lead to a decision (Müller & Guido, 2017). A major complication in modelling with decision trees is avoiding overfitting. A tree that grows arbitrarily complex can trivially fit any dataset by simply having a branch for each datapoint, leading to 100% accuracy for the training-set. But very poor generalization (Müller & Guido, 2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flare class:  C | Pearson R = 0.154 | two - tailed p = 4.77e-02 \n",
      "Flare class:  M | Pearson R = 0.105 | two - tailed p = 1.78e-01 \n",
      "Flare class:  X | Pearson R = -0.012 | two - tailed p = 8.83e-01 \n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state = 0)\n",
    "tree.fit(X_train, Y_train)\n",
    "predicted_vals = tree.predict(X_test)\n",
    "\n",
    "a = evaluate_prediction(predicted_vals, Y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptrons\n",
    "Resurging in the late 90's multilayer perceptrons have achieved stellar results in both classification and regression. The algorithm is based on applying multiple sets of non-linear transformations to the data to be able to classify or preform regression on complex boundaries. At the heart of the algorithm is the minimization of a cost-function and the iterative updating of the weights by way of back-propagating the error in the outermost layer. Overfitting can be a problem in MLPs, but the consequence is generally less severe than for a decision tree. One primary concern for the MLP is the shape and distribution of the input-data. One of the most applied MLP algorithms \"adam\" is, for example, very sensitive to this. Generally one should seek to have each input with mean 0 and unit variance (much of the math is matric multiplication, unevenly scaled variables can create large problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flare class:  C | Pearson R = 0.332 | two - tailed p = 1.20e-05 \n",
      "Flare class:  M | Pearson R = 0.336 | two - tailed p = 9.73e-06 \n",
      "Flare class:  X | Pearson R = 0.268 | two - tailed p = 4.85e-04 \n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes = 10, \n",
    "                   solver = \"lbfgs\", \n",
    "                   learning_rate = \"adaptive\", \n",
    "                   max_iter = 800,)\n",
    "mlp.fit(X_train, Y_train)\n",
    "predicted_vals_mlp = mlp.predict(X_test)\n",
    "\n",
    "a = evaluate_prediction(predicted_vals_mlp, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "As predicted the predictive powers of these models are sub-par, we barely achieved statistically significant results for the C and M class flares and were able to tell nothing useful for the X class. Now let's see if we can get clever. \n",
    "\n",
    "# Considerations\n",
    "\n",
    "### Output representation\n",
    "In the dataset the number of C, M and X-class flares are integer numbered, but accurately prediciting this integer turns out to be tricky. Instead we might be interested in classifying if the number of flares for a given class is greater than or equal to 0. \n",
    "\n",
    "### Sampling\n",
    "One popular method of error-minimization is some manner of _folding_ the data. Folding generally consists of making sure the model is trained on the whole dataset while still maintaining the ability to generalize. A vital part of this process is  how the data is sampled, a common tactic in computational statistic is to employ **stratified sampling** to ensure that the training and test-sets are balanced in the occurances of anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "A. Müller and S. Guido, _Introduction to Machine Learning with Python_. O'reilly media, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
